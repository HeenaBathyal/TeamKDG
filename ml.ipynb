{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy scikit-learn sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L8VxSvZJyJR",
        "outputId": "b816f923-1316-49ad-aeec-b97bbd795b79"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ── 1. Load data\n",
        "df = pd.read_csv('/indian_dummy_user_data.csv')\n",
        "\n",
        "# ── 2. Ensure all necessary text columns exist\n",
        "survey_cols = ['Vibe', 'Cleaning', 'Chatting', 'Noise Comfort', 'Work Setup', 'Tone']\n",
        "for col in survey_cols:\n",
        "    if col not in df.columns:\n",
        "        df[col] = \"\"\n",
        "\n",
        "# ── 3. Combine all text responses into one field\n",
        "df['full_text'] = df[survey_cols].fillna('').apply(lambda row: \" | \".join(row.astype(str)), axis=1)\n",
        "\n",
        "# ── 4. Structured (categorical) features\n",
        "structured_cols = ['Diet', 'Personality', 'Sleep Habit', 'Noise Tolerance', 'Smoke Alcohol']\n",
        "enc_struct = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "X_struct_raw = enc_struct.fit_transform(df[structured_cols].astype(str))\n",
        "\n",
        "# Normalize structured data\n",
        "struct_scaler = MinMaxScaler()\n",
        "X_struct = struct_scaler.fit_transform(X_struct_raw)\n",
        "\n",
        "# ── 5. Encode full_text using SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "emb_struct = model.encode(df['full_text'].tolist(), convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "# ── 6. Compatibility scoring between 2 users\n",
        "def compatibility_score(idx_a, idx_b, w=(0.7, 0.3)):\n",
        "    xa, xb = emb_struct[idx_a:idx_a+1], emb_struct[idx_b:idx_b+1]\n",
        "    xs, ys = X_struct[idx_a:idx_a+1], X_struct[idx_b:idx_b+1]\n",
        "    t_sim = cosine_similarity(xa, xb)[0][0]\n",
        "    s_sim = cosine_similarity(xs, ys)[0][0]\n",
        "    return round(float(w[0] * t_sim + w[1] * s_sim) * 100, 1)\n",
        "\n",
        "# ── 7. Example: Top matches for a user at index 0\n",
        "base_idx = 5\n",
        "scores = [(i, compatibility_score(base_idx, i)) for i in range(len(df)) if i != base_idx]\n",
        "scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "print(f\"Top matches for user #{base_idx}:\")\n",
        "for i, sc in scores[:5]:\n",
        "    print(f\"• {df.loc[i, 'First Name']} {df.loc[i, 'Last Name']}: {sc}% match\")\n",
        "\n",
        "# ── 8. New user scoring + match explanations\n",
        "def score_new_user(user_q: dict, user_struct: dict, text_w=0.7, struct_w=0.3):\n",
        "    # Build user text and embed\n",
        "    utt = \" | \".join([user_q.get(k, \"\") for k in survey_cols])\n",
        "    emb_user = model.encode([utt], normalize_embeddings=True)[0]\n",
        "\n",
        "    # Structured encoding\n",
        "    vec_struct = enc_struct.transform([[user_struct.get(k, \"\") for k in structured_cols]])\n",
        "    vec_struct_scaled = struct_scaler.transform(vec_struct)\n",
        "\n",
        "    # Similarities\n",
        "    sim_text = cosine_similarity(emb_user.reshape(1, -1), emb_struct)[0]\n",
        "    sim_struct = cosine_similarity(vec_struct_scaled.reshape(1, -1), X_struct)[0]\n",
        "    final = np.clip(sim_text, 0, 1) * text_w + sim_struct * struct_w\n",
        "\n",
        "    # Explanation of shared structured traits\n",
        "    reasons = []\n",
        "    for i in range(len(df)):\n",
        "        shared = [col for col in structured_cols\n",
        "                  if user_struct.get(col, \"\").lower() == str(df.loc[i, col]).lower()]\n",
        "        reason = \"Similar in: \" + (\", \".join(shared) if shared else \"Mostly vibe-based match\")\n",
        "        reasons.append(reason)\n",
        "\n",
        "    result = pd.DataFrame({\n",
        "        'First Name': df['First Name'],\n",
        "        'Last Name': df['Last Name'],\n",
        "        'Compatibility (%)': np.round(final * 100, 1),\n",
        "        'Vibe': df['Vibe'],\n",
        "        'Why Match?': reasons\n",
        "    }).sort_values('Compatibility (%)', ascending=False)\n",
        "\n",
        "    return result.reset_index(drop=True)\n",
        "\n",
        "# ── Example new user\n",
        "new_q = {\n",
        "    'Vibe': 'creative and messy',\n",
        "    'Cleaning': 'super organized',\n",
        "    'Chatting': 'prefers quiet space',\n",
        "    'Noise Comfort': 'soft like a library',\n",
        "    'Work Setup': 'office commute daily',\n",
        "    'Tone': 'friendly and chill'\n",
        "}\n",
        "new_struct = {\n",
        "    'Diet': 'Vegetarian',\n",
        "    'Personality': 'Ambivert',\n",
        "    'Sleep Habit': 'Early Bird',\n",
        "    'Noise Tolerance': 'Low',\n",
        "    'Smoke Alcohol': 'No'\n",
        "}\n",
        "\n",
        "# Run match\n",
        "top = score_new_user(new_q, new_struct)\n",
        "print(\"\\nTop 10 matches for the new user:\")\n",
        "print(top.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtoW6anx2LZv",
        "outputId": "e5eb1b86-0239-424c-ce8f-b7f2f28fb832"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matches for user #5:\n",
            "• Bhavin Singhal: 94.0% match\n",
            "• Vedika Krish: 94.0% match\n",
            "• Anahita Kurian: 94.0% match\n",
            "• Amira Kala: 94.0% match\n",
            "• Biju Mangal: 94.0% match\n",
            "\n",
            "Top 10 matches for the new user:\n",
            "  First Name      Last Name  Compatibility (%) Vibe  \\\n",
            "0      Anahi          Arora               37.2        \n",
            "1       Riya          Manda               37.2        \n",
            "2       Urvi          Shere               37.2        \n",
            "3        Ela  Krishnamurthy               37.2        \n",
            "4      Kabir            Dar               37.2        \n",
            "5      Zaina           Kaur               37.2        \n",
            "6      Gatik     Srinivasan               37.2        \n",
            "7  Dharmajan      Choudhury               29.5        \n",
            "8      Jayan           Toor               29.5        \n",
            "9     Himmat          Kumar               29.5        \n",
            "\n",
            "                                          Why Match?  \n",
            "0  Similar in: Personality, Noise Tolerance, Smok...  \n",
            "1  Similar in: Personality, Noise Tolerance, Smok...  \n",
            "2  Similar in: Personality, Noise Tolerance, Smok...  \n",
            "3  Similar in: Personality, Noise Tolerance, Smok...  \n",
            "4  Similar in: Personality, Noise Tolerance, Smok...  \n",
            "5  Similar in: Personality, Noise Tolerance, Smok...  \n",
            "6  Similar in: Personality, Noise Tolerance, Smok...  \n",
            "7             Similar in: Personality, Smoke Alcohol  \n",
            "8           Similar in: Personality, Noise Tolerance  \n",
            "9             Similar in: Personality, Smoke Alcohol  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NSW1Qw8HLhD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGfU86kfAjHn",
        "outputId": "a7760eb0-f0a8-41e4-b31c-beaedaf43cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample data + voice .wav files created under: SarthiApp_demo\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Create folder structure\n",
        "root = \"SarthiApp_demo\"\n",
        "voice_dir = os.path.join(root, \"voice_samples\")\n",
        "os.makedirs(voice_dir, exist_ok=True)\n",
        "\n",
        "# 2. Define sample dataset rows\n",
        "rows = [\n",
        "    [\"Riya\", \"Goel\", \"creative and messy\", \"super organized\", \"prefers quiet space\", \"soft like a library\", \"office commute daily\", \"friendly and chill\", \"Vegetarian\", \"Ambivert\", \"Early Bird\", \"Low\", \"No\", \"voice_samples/user_0.wav\"],\n",
        "    [\"Ananya\", \"Sharma\", \"fun-loving and active\", \"ok with a little mess\", \"chatty and sociable\", \"lively and vibrant\", \"WFH setup\", \"enthusiastic and expressive\", \"Non-Vegetarian\", \"Extrovert\", \"Night Owl\", \"High\", \"Yes\", \"voice_samples/user_1.wav\"],\n",
        "    [\"Neha\", \"Verma\", \"calm and focused\", \"clean and tidy\", \"quiet\", \"calm and cozy\", \"hybrid worker\", \"calm and mature\", \"Jain\", \"Introvert\", \"Early Bird\", \"Medium\", \"No\", \"voice_samples/user_2.wav\"],\n",
        "    [\"Mihir\", \"Patel\", \"laid back\", \"cleaner than most\", \"loves deep convos\", \"white noise fan\", \"remote full-time\", \"chill and funny\", \"Eggitarian\", \"Ambivert\", \"Night Owl\", \"High\", \"No\", \"voice_samples/user_3.wav\"],\n",
        "    [\"Simran\", \"Kaur\", \"quiet but warm\", \"super clean\", \"soft talker\", \"needs pin drop silence\", \"hybrid setup\", \"shy but sweet\", \"Vegan\", \"Introvert\", \"Early Bird\", \"Low\", \"No\", \"voice_samples/user_4.wav\"],\n",
        "    [\"Arjun\", \"Mehta\", \"energized and loud\", \"not very tidy\", \"talks a lot\", \"noisy with music\", \"WFH with music pump\", \"loud and extrovert\", \"Non-Vegetarian\", \"Extrovert\", \"Night Owl\", \"Very High\", \"Yes\", \"voice_samples/user_5.wav\"],\n",
        "]\n",
        "\n",
        "csv_path = os.path.join(root, \"sample_user_data.csv\")\n",
        "\n",
        "# 3. Write CSV\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    header = [\"First Name\",\"Last Name\",\"Vibe\",\"Cleaning\",\"Chatting\",\"Noise Comfort\",\"Work Setup\",\"Tone\",\"Diet\",\"Personality\",\"Sleep Habit\",\"Noise Tolerance\",\"Smoke Alcohol\",\"Voice Path\"]\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(rows)\n",
        "\n",
        "# 4. Generate synthetic .wav files (sine waves at different frequencies)\n",
        "sr = 16000\n",
        "duration = 3.0\n",
        "\n",
        "for idx, freq in enumerate([200, 300, 400, 500, 600, 700]):\n",
        "    t = np.linspace(0, duration, int(sr * duration), endpoint=False)\n",
        "    sine = 0.5 * np.sin(2 * np.pi * freq * t)\n",
        "    path = os.path.join(root, \"voice_samples\", f\"user_{idx}.wav\")\n",
        "    sf.write(path, sine, sr)\n",
        "\n",
        "print(\"Sample data + voice .wav files created under:\", root)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xA1Q2waiLnmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install speechbrain torchaudio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4o1wA9vBaZo",
        "outputId": "55488261-733e-423f-db7d-75b909e9dde1"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting speechbrain\n",
            "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting hyperpyyaml (from speechbrain)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from speechbrain) (25.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from speechbrain) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->speechbrain) (1.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (1.1.5)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->speechbrain) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (2025.7.14)\n",
            "Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml.clib, ruamel.yaml, hyperpyyaml, speechbrain\n",
            "Successfully installed hyperpyyaml-1.2.2 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 speechbrain-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "import torchaudio\n",
        "\n",
        "# Load sample dataset\n",
        "df = pd.read_csv(\"SarthiApp_demo/sample_user_data.csv\")\n",
        "\n",
        "# Build structured features\n",
        "survey_cols = ['Vibe', 'Cleaning', 'Chatting', 'Noise Comfort', 'Work Setup', 'Tone']\n",
        "structured_cols = ['Diet','Personality','Sleep Habit','Noise Tolerance','Smoke Alcohol']\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "X_struct = enc.fit_transform(df[structured_cols].astype(str))\n",
        "X_struct = MinMaxScaler().fit_transform(X_struct)\n",
        "\n",
        "# Text embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "emb_text = model.encode(df[survey_cols].fillna('').apply(lambda row: \" | \".join(row), axis=1).tolist(),\n",
        "                        convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "# Voice embeddings from SpeechBrain\n",
        "voice_model = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
        "\n",
        "def extract_voice_emb(path):\n",
        "    signal, fs = torchaudio.load(path)\n",
        "    if fs != 16000:\n",
        "        signal = torchaudio.transforms.Resample(orig_freq=fs, new_freq=16000)(signal)\n",
        "    emb = voice_model.encode_batch(signal).squeeze().detach().cpu().numpy()\n",
        "    return emb / np.linalg.norm(emb)\n",
        "\n",
        "voice_emb = []\n",
        "for p in df['Voice Path']:\n",
        "    full = os.path.join(\"SarthiApp_demo\", p)\n",
        "    if os.path.exists(full):\n",
        "        voice_emb.append(extract_voice_emb(full))\n",
        "    else:\n",
        "        voice_emb.append(np.zeros((192,)))\n",
        "voice_emb = np.array(voice_emb)\n",
        "\n",
        "# Scoring functions\n",
        "def comp(idx_a, idx_b):\n",
        "    t = cosine_similarity(emb_text[idx_a:idx_a+1], emb_text[idx_b:idx_b+1])[0][0]\n",
        "    s = cosine_similarity(X_struct[idx_a:idx_a+1], X_struct[idx_b:idx_b+1])[0][0]\n",
        "    v = cosine_similarity(voice_emb[idx_a:idx_a+1], voice_emb[idx_b:idx_b+1])[0][0]\n",
        "    return round((0.5*t + 0.3*s + 0.2*v) * 100, 1)\n",
        "\n",
        "# Example: top matches for user 0\n",
        "scores = [(i, comp(0, i)) for i in range(len(df)) if i != 0]\n",
        "scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "print(\"Top matches for user 5:\")\n",
        "for i, sc in scores[:5]:\n",
        "    print(f\"• {df.loc[i,'First Name']} {df.loc[i,'Last Name']}: {sc}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6pvLHRpA5wz",
        "outputId": "5c0607c9-6dc7-404a-b6f9-1a0ef18c94e3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
            "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Fetching files for pretraining (no collection directory set)\n",
            "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/embedding_model.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"mean_var_norm_emb\"] = /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/mean_var_norm_emb.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/classifier.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"label_encoder\"] = /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/label_encoder.txt\n",
            "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/embedding_model.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): mean_var_norm_emb -> /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/mean_var_norm_emb.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/classifier.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): label_encoder -> /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/label_encoder.txt\n",
            "DEBUG:speechbrain.dataio.encoder:Loaded categorical encoding from /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/label_encoder.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matches for user 5:\n",
            "• Neha Verma: 60.5%\n",
            "• Simran Kaur: 59.7%\n",
            "• Mihir Patel: 57.0%\n",
            "• Ananya Sharma: 45.1%\n",
            "• Arjun Mehta: 42.7%\n"
          ]
        }
      ]
    }
  ]
}